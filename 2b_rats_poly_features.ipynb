{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive modelling with Python\n",
    "\n",
    "*Jure Å½abkar*\n",
    "\n",
    "*Wed, 5 March 2025*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rcParams['figure.figsize'] = [10, 7]\n",
    "plt.rcParams['figure.dpi'] = 150 # makes figures bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the initial data set with only 10 samples, let's go rat hunting and collect more data. We store the data in the file and create a new scatter plot for the entire rat collection.\n",
    "Visualizing data helps us gain more insight: rat size increases with increasing weight, but really fat rats are not the biggest. A possible explanation would be, for example, that excessive weight is due to genetic or health related problem, not the size of the rat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rats = pd.read_csv('datasets/rats-obesity-100examples.csv')\n",
    "\n",
    "rats.plot.scatter(x='weight', y='size');\n",
    "plt.xlabel(\"weight [dag]\");\n",
    "plt.ylabel(\"size [cm]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we had enough rat hunting, this is all the data we could collect.\n",
    "Our next step will be to put some data aside, so that we could learn/train our model on one subset and test it on another one. It is very important to train and test on different data sets.\n",
    "There are many possible ways of how we can do this; this time we will take 70% of our data foir training and the remaining 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(rats[\"weight\"], rats[\"size\"], test_size=0.3, random_state=42);\n",
    "\n",
    "plt.plot(X_train, y_train, \"b.\", label=\"training data\");\n",
    "plt.plot(X_test, y_test, \"r.\", label=\"test data\");\n",
    "plt.xlabel(\"weight [dag]\");\n",
    "plt.ylabel(\"size [cm]\");\n",
    "plt.title(\"Rats\");\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between weight and size indicates a strong relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_train</th>\n",
       "      <th>y_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X_train</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.73403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_train</th>\n",
       "      <td>0.73403</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X_train  y_train\n",
       "X_train  1.00000  0.73403\n",
       "y_train  0.73403  1.00000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data={\"X_train\":X_train, \"y_train\":y_train})\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next goal is to use the **training set** to build the model for prediction of rat's size, given its weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression model:\n",
      "y = 2.18 x + 12.16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train = X_train.values.reshape(-1,1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print('Linear regression model:\\ny = {:.2f} x + {:.2f}'.format(lin_reg.coef_[0], lin_reg.intercept_));\n",
    "plt.plot(X_train, y_train, \"b.\", label=\"training data\")\n",
    "xlr = [min(X_train), max(X_train)]\n",
    "ylr = lin_reg.predict(xlr)\n",
    "plt.plot(xlr, ylr, \"k-\", label=\"linear regression model\")\n",
    "plt.plot(X_test, y_test, \"ro\", label=\"test data\")\n",
    "\n",
    "plt.xlabel(\"weight [dag]\");\n",
    "plt.ylabel(\"size [cm]\");\n",
    "#plt.title(\"Linear regression: High bias, low variance\");\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained model is obviously too simple to catch the complexity of the underlying data. We say it has a high bias.\n",
    "\n",
    "**BIAS** = Average prediction of the model - the correct value\n",
    "\n",
    "Model with high bias doesn't care much about the training data, oversimplifies the model.\n",
    "\n",
    "A model with high bias leads to high error on training and test data. We can verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on TEST data:\n",
      "Mean squared error: 8.90\n",
      "Variance score: 0.56\n",
      "\n",
      "Model performance on TRAIN data:\n",
      "Mean squared error: 12.61\n",
      "Variance score: 0.54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "print(\"Model performance on TEST data:\")\n",
    "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(y_test, y_pred) ))\n",
    "print('Variance score: {:.2f}\\n'.format(r2_score(y_test, y_pred) ))\n",
    "\n",
    "y_pred_train = lin_reg.predict(X_train)\n",
    "print(\"Model performance on TRAIN data:\")\n",
    "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(y_train, y_pred_train) ))\n",
    "print('Variance score: {:.2f}'.format(r2_score(y_train, y_pred_train) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the bias we should learn a more complex model.\n",
    "\n",
    "What can we do to enable linear regression to come up with something better? One way is to construct new features: what kind of features would be useful here?\n",
    "\n",
    "A domain expert is usually the right person to talk to when we start thinking about constructing new features. In our simple 2D example, we can be good domain experts ourselves.\n",
    "\n",
    "A higher degree polinomial would certainly fit the training data better than the above linear model. It seems that would solve our problem, so let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 5.11131700e+00, 2.61255615e+01, 1.33536027e+02,\n",
       "        6.82544964e+02, 3.48870368e+03, 1.78318704e+04, 9.11443425e+04],\n",
       "       [1.00000000e+00, 2.76168489e+00, 7.62690343e+00, 2.10631040e+01,\n",
       "        5.81696560e+01, 1.60646260e+02, 4.43654349e+02, 1.22523351e+03],\n",
       "       [1.00000000e+00, 6.37531731e+00, 4.06446708e+01, 2.59122673e+02,\n",
       "        1.65198926e+03, 1.05319557e+04, 6.71445598e+04, 4.28067874e+05],\n",
       "       [1.00000000e+00, 2.01898252e+00, 4.07629041e+00, 8.22995906e+00,\n",
       "        1.66161435e+01, 3.35477032e+01, 6.77322262e+01, 1.36750181e+02],\n",
       "       [1.00000000e+00, 4.47033769e+00, 1.99839190e+01, 8.93348664e+01,\n",
       "        3.99357020e+02, 1.78526074e+03, 7.98071835e+03, 3.56765060e+04],\n",
       "       [1.00000000e+00, 1.55403157e+00, 2.41501412e+00, 3.75300818e+00,\n",
       "        5.83229318e+00, 9.06356772e+00, 1.40850704e+01, 2.18886440e+01],\n",
       "       [1.00000000e+00, 4.98276787e+00, 2.48279757e+01, 1.23712039e+02,\n",
       "        6.16428375e+02, 3.07151950e+03, 1.53046687e+04, 7.62596115e+04],\n",
       "       [1.00000000e+00, 4.21537844e+00, 1.77694154e+01, 7.49048103e+01,\n",
       "        3.15752122e+02, 1.33101469e+03, 5.61073061e+03, 2.36513528e+04],\n",
       "       [1.00000000e+00, 2.89309379e+00, 8.36999165e+00, 2.42151708e+01,\n",
       "        7.00567603e+01, 2.02680778e+02, 5.86374499e+02, 1.69643642e+03],\n",
       "       [1.00000000e+00, 3.50382881e+00, 1.22768164e+01, 4.30158629e+01,\n",
       "        1.50720220e+02, 5.28097849e+02, 1.85036446e+03, 6.48336032e+03],\n",
       "       [1.00000000e+00, 1.78017143e+00, 3.16901033e+00, 5.64138166e+00,\n",
       "        1.00426265e+01, 1.78775968e+01, 3.18251870e+01, 5.66542888e+01],\n",
       "       [1.00000000e+00, 5.15126268e+00, 2.65355072e+01, 1.36691368e+02,\n",
       "        7.04133144e+02, 3.62717479e+03, 1.86845301e+04, 9.62489229e+04],\n",
       "       [1.00000000e+00, 3.07336436e+00, 9.44556850e+00, 2.90296736e+01,\n",
       "        8.92187643e+01, 2.74201771e+02, 8.42721950e+02, 2.58999161e+03],\n",
       "       [1.00000000e+00, 2.42216188e+00, 5.86686818e+00, 1.42105045e+01,\n",
       "        3.44201422e+01, 8.33711565e+01, 2.01938437e+02, 4.89127585e+02],\n",
       "       [1.00000000e+00, 1.23432870e+00, 1.52356734e+00, 1.88058289e+00,\n",
       "        2.32125743e+00, 2.86519467e+00, 3.53659201e+00, 4.36531702e+00],\n",
       "       [1.00000000e+00, 2.18860893e+00, 4.79000907e+00, 1.04834566e+01,\n",
       "        2.29441869e+01, 5.02158524e+01, 1.09902863e+02, 2.40534388e+02],\n",
       "       [1.00000000e+00, 4.74203324e+00, 2.24868793e+01, 1.06633529e+02,\n",
       "        5.05659739e+02, 2.39785529e+03, 1.13707095e+04, 5.39202825e+04],\n",
       "       [1.00000000e+00, 6.36763998e+00, 4.05468389e+01, 2.58187673e+02,\n",
       "        1.64404615e+03, 1.04686940e+04, 6.66608743e+04, 4.24472448e+05],\n",
       "       [1.00000000e+00, 5.16640095e+00, 2.66916987e+01, 1.37900018e+02,\n",
       "        7.12446782e+02, 3.68078573e+03, 1.90164149e+04, 9.82464237e+04],\n",
       "       [1.00000000e+00, 6.26870462e+00, 3.92966576e+01, 2.46339139e+02,\n",
       "        1.54422730e+03, 9.68030479e+03, 6.06829714e+04, 3.80403623e+05],\n",
       "       [1.00000000e+00, 6.25833491e+00, 3.91667559e+01, 2.45118676e+02,\n",
       "        1.53403477e+03, 9.60050334e+03, 6.00831653e+04, 3.76020571e+05],\n",
       "       [1.00000000e+00, 2.81399544e+00, 7.91857031e+00, 2.22828207e+01,\n",
       "        6.27037558e+01, 1.76448083e+02, 4.96524099e+02, 1.39721655e+03],\n",
       "       [1.00000000e+00, 4.35213897e+00, 1.89411136e+01, 8.24343587e+01,\n",
       "        3.58765785e+02, 1.56139855e+03, 6.79542350e+03, 2.95746274e+04],\n",
       "       [1.00000000e+00, 1.10972966e+00, 1.23149993e+00, 1.36663200e+00,\n",
       "        1.51659207e+00, 1.68300721e+00, 1.86768303e+00, 2.07262326e+00],\n",
       "       [1.00000000e+00, 3.38060485e+00, 1.14284891e+01, 3.86352057e+01,\n",
       "        1.30610364e+02, 4.41542028e+02, 1.49267912e+03, 5.04615826e+03],\n",
       "       [1.00000000e+00, 5.50487262e+00, 3.03036225e+01, 1.66817582e+02,\n",
       "        9.18309540e+02, 5.05517704e+03, 2.78281057e+04, 1.53190177e+05],\n",
       "       [1.00000000e+00, 2.11756127e+00, 4.48406572e+00, 9.49528390e+00,\n",
       "        2.01068454e+01, 4.25774771e+01, 9.01604164e+01, 1.90920206e+02],\n",
       "       [1.00000000e+00, 1.29972075e+00, 1.68927404e+00, 2.19558453e+00,\n",
       "        2.85364677e+00, 3.70894394e+00, 4.82059141e+00, 6.26542270e+00],\n",
       "       [1.00000000e+00, 5.11900557e+00, 2.62042180e+01, 1.34139538e+02,\n",
       "        6.86661040e+02, 3.51502169e+03, 1.79934156e+04, 9.21083945e+04],\n",
       "       [1.00000000e+00, 4.73017432e+00, 2.23745491e+01, 1.05835518e+02,\n",
       "        5.00620448e+02, 2.36802199e+03, 1.12011568e+04, 5.29834242e+04],\n",
       "       [1.00000000e+00, 4.44470563e+00, 1.97554082e+01, 8.78069739e+01,\n",
       "        3.90276152e+02, 1.73466261e+03, 7.71006467e+03, 3.42689679e+04],\n",
       "       [1.00000000e+00, 4.70286948e+00, 2.21169814e+01, 1.04013277e+02,\n",
       "        4.89160865e+02, 2.30045970e+03, 1.08187617e+04, 5.08792243e+04],\n",
       "       [1.00000000e+00, 3.94943896e+00, 1.55980681e+01, 6.16036176e+01,\n",
       "        2.43299727e+02, 9.60897421e+02, 3.79500571e+03, 1.49881434e+04],\n",
       "       [1.00000000e+00, 5.73567597e+00, 3.28979788e+01, 1.88692147e+02,\n",
       "        1.08227701e+03, 6.20759025e+03, 3.56047262e+04, 2.04217173e+05],\n",
       "       [1.00000000e+00, 1.11620175e+00, 1.24590634e+00, 1.39068283e+00,\n",
       "        1.55228261e+00, 1.73266056e+00, 1.93399875e+00, 2.15873278e+00],\n",
       "       [1.00000000e+00, 4.08933467e+00, 1.67226581e+01, 6.83845454e+01,\n",
       "        2.79647293e+02, 1.14357137e+03, 4.67644605e+03, 1.91235530e+04],\n",
       "       [1.00000000e+00, 6.45157302e+00, 4.16227944e+01, 2.68532497e+02,\n",
       "        1.73245701e+03, 1.11770729e+04, 7.21097022e+04, 4.65221009e+05],\n",
       "       [1.00000000e+00, 6.66756854e+00, 4.44564702e+01, 2.96416562e+02,\n",
       "        1.97637774e+03, 1.31776340e+04, 8.78627781e+04, 5.85831095e+05],\n",
       "       [1.00000000e+00, 3.48433593e+00, 1.21405969e+01, 4.23019178e+01,\n",
       "        1.47394092e+02, 5.13570530e+02, 1.78945225e+03, 6.23505276e+03],\n",
       "       [1.00000000e+00, 4.44207692e+00, 1.97320474e+01, 8.76512722e+01,\n",
       "        3.89353693e+02, 1.72953905e+03, 7.68274551e+03, 3.41273465e+04],\n",
       "       [1.00000000e+00, 5.35598791e+00, 2.86866065e+01, 1.53645118e+02,\n",
       "        8.22921393e+02, 4.40755704e+03, 2.36068222e+04, 1.26437854e+05],\n",
       "       [1.00000000e+00, 5.48899393e+00, 3.01290543e+01, 1.65378196e+02,\n",
       "        9.07759914e+02, 4.98268866e+03, 2.73499478e+04, 1.50123697e+05],\n",
       "       [1.00000000e+00, 4.53583322e+00, 2.05737830e+01, 9.33192485e+01,\n",
       "        4.23280547e+02, 1.91992997e+03, 8.70848214e+03, 3.95002226e+04],\n",
       "       [1.00000000e+00, 2.72665203e+00, 7.43463130e+00, 2.02716525e+01,\n",
       "        5.52737426e+01, 1.50712262e+02, 4.10939897e+02, 1.12049010e+03],\n",
       "       [1.00000000e+00, 1.01722196e+00, 1.03474052e+00, 1.05256078e+00,\n",
       "        1.07068794e+00, 1.08912729e+00, 1.10788420e+00, 1.12696414e+00],\n",
       "       [1.00000000e+00, 1.88037145e+00, 3.53579679e+00, 6.64861133e+00,\n",
       "        1.25018589e+01, 2.35081386e+01, 4.42040326e+01, 8.31200009e+01],\n",
       "       [1.00000000e+00, 1.99212518e+00, 3.96856274e+00, 7.90587378e+00,\n",
       "        1.57494902e+01, 3.13749561e+01, 6.25028402e+01, 1.24513482e+02],\n",
       "       [1.00000000e+00, 6.74733718e+00, 4.55265590e+01, 3.07183044e+02,\n",
       "        2.07266758e+03, 1.39849870e+04, 9.43614228e+04, 6.36688337e+05],\n",
       "       [1.00000000e+00, 3.44882082e+00, 1.18943650e+01, 4.10215337e+01,\n",
       "        1.41475919e+02, 4.87925096e+02, 1.68276623e+03, 5.80355919e+03],\n",
       "       [1.00000000e+00, 5.19855016e+00, 2.70249238e+01, 1.40490422e+02,\n",
       "        7.30346505e+02, 3.79674294e+03, 1.97375586e+04, 1.02606688e+05],\n",
       "       [1.00000000e+00, 3.48507562e+00, 1.21457521e+01, 4.23288643e+01,\n",
       "        1.47519293e+02, 5.14115891e+02, 1.79173276e+03, 6.24432414e+03],\n",
       "       [1.00000000e+00, 2.61956735e+00, 6.86213310e+00, 1.79758198e+01,\n",
       "        4.70888707e+01, 1.23352468e+02, 3.23130099e+02, 8.46461057e+02],\n",
       "       [1.00000000e+00, 6.00775403e+00, 3.60931085e+01, 2.16838518e+02,\n",
       "        1.30271248e+03, 7.82637616e+03, 4.70189429e+04, 2.82478244e+05],\n",
       "       [1.00000000e+00, 6.26885502e+00, 3.92985433e+01, 2.46356870e+02,\n",
       "        1.54437550e+03, 9.68146613e+03, 6.06917075e+04, 3.80467515e+05],\n",
       "       [1.00000000e+00, 5.32194696e+00, 2.83231195e+01, 1.50734139e+02,\n",
       "        8.02199095e+02, 4.26926104e+03, 2.27207808e+04, 1.20918790e+05],\n",
       "       [1.00000000e+00, 2.26976870e+00, 5.15184993e+00, 1.16935077e+01,\n",
       "        2.65415577e+01, 6.02431969e+01, 1.36738122e+02, 3.10363910e+02],\n",
       "       [1.00000000e+00, 6.80956945e+00, 4.63702362e+01, 3.15761344e+02,\n",
       "        2.15019880e+03, 1.46419281e+04, 9.97052262e+04, 6.78949663e+05],\n",
       "       [1.00000000e+00, 1.00068625e+00, 1.00137297e+00, 1.00206016e+00,\n",
       "        1.00274782e+00, 1.00343596e+00, 1.00412456e+00, 1.00481364e+00],\n",
       "       [1.00000000e+00, 5.15393569e+00, 2.65630531e+01, 1.36904268e+02,\n",
       "        7.05595792e+02, 3.63659534e+03, 1.87427785e+04, 9.65990752e+04],\n",
       "       [1.00000000e+00, 6.78904028e+00, 4.60910680e+01, 3.12914117e+02,\n",
       "        2.12438655e+03, 1.44225458e+04, 9.79152447e+04, 6.64750540e+05],\n",
       "       [1.00000000e+00, 6.69693555e+00, 4.48489458e+01, 3.00350500e+02,\n",
       "        2.01142794e+03, 1.34704033e+04, 9.02104226e+04, 6.04133386e+05],\n",
       "       [1.00000000e+00, 3.38606102e+00, 1.14654092e+01, 3.88225753e+01,\n",
       "        1.31455609e+02, 4.45116714e+02, 1.50719236e+03, 5.10344529e+03],\n",
       "       [1.00000000e+00, 3.56854714e+00, 1.27345287e+01, 4.54437659e+01,\n",
       "        1.62168221e+02, 5.78704941e+02, 2.06513586e+03, 7.36953466e+03],\n",
       "       [1.00000000e+00, 5.50565460e+00, 3.03122326e+01, 1.66888683e+02,\n",
       "        9.18831446e+02, 5.05876858e+03, 2.78518325e+04, 1.53342570e+05],\n",
       "       [1.00000000e+00, 5.80446741e+00, 3.36918419e+01, 1.95563199e+02,\n",
       "        1.13514021e+03, 6.58888438e+03, 3.82449646e+04, 2.21991651e+05],\n",
       "       [1.00000000e+00, 1.61400657e+00, 2.60501722e+00, 4.20451491e+00,\n",
       "        6.78611470e+00, 1.09528337e+01, 1.76779456e+01, 2.85323205e+01],\n",
       "       [1.00000000e+00, 1.82484822e+00, 3.33007104e+00, 6.07687423e+00,\n",
       "        1.10893732e+01, 2.02364229e+01, 3.69284004e+01, 6.73887260e+01],\n",
       "       [1.00000000e+00, 1.16432556e+00, 1.35565401e+00, 1.57842261e+00,\n",
       "        1.83779779e+00, 2.13979494e+00, 2.49141794e+00, 2.90082158e+00],\n",
       "       [1.00000000e+00, 3.69947280e+00, 1.36860990e+01, 5.06313510e+01,\n",
       "        1.87309306e+02, 6.92945683e+02, 2.56353371e+03, 9.48372322e+03],\n",
       "       [1.00000000e+00, 5.07301320e+00, 2.57354629e+01, 1.30556343e+02,\n",
       "        6.62314051e+02, 3.35992792e+03, 1.70449587e+04, 8.64693004e+04]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=7)\n",
    "poly.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, we introduced a very useful feature of sklearn - **the pipeline**. The pipeline helps us in constructing data transformations so that we can carry them out on test data in the exactly the same way as we do on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_pipe = Pipeline([('poly', PolynomialFeatures(degree=3)),\n",
    "                  ('linear', LinearRegression())])\n",
    "poly = poly_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on TEST data:\n",
      "Mean squared error: 3.65\n",
      "Variance score: 0.82\n",
      "\n",
      "Model performance on TRAIN data:\n",
      "Mean squared error: 3.58\n",
      "Variance score: 0.87\n"
     ]
    }
   ],
   "source": [
    "y_pred_poly = poly.predict(X_test);\n",
    "plt.plot(X_train, y_train, \"b.\", label=\"training data\");\n",
    "\n",
    "X_test4 = np.linspace(1, 6.8, 1000)\n",
    "X_test4 = X_test4.reshape(-1, 1)\n",
    "y_test4 = poly.predict(X_test4)\n",
    "\n",
    "plt.plot(X_test4, y_test4, \"k-\", label=\"13th degree polynomial model\")\n",
    "plt.plot(X_test, y_test, \"ro\", label=\"test data\")\n",
    "\n",
    "plt.xlabel(\"weight [dag]\")\n",
    "plt.ylabel(\"size [cm]\")\n",
    "\n",
    "print(\"Model performance on TEST data:\")\n",
    "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(y_test, y_pred_poly) ))\n",
    "print('Variance score: {:.2f}\\n'.format(r2_score(y_test, y_pred_poly) ))\n",
    "\n",
    "y_pred_train = poly.predict(X_train)\n",
    "print(\"Model performance on TRAIN data:\")\n",
    "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(y_train, y_pred_train) ))\n",
    "print('Variance score: {:.2f}'.format(r2_score(y_train, y_pred_train) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high-degree polynomial effectively reduces the bias, but at the same time increases the variance.\n",
    "\n",
    "**Variance** is the variability of model prediction for a given data point. Model with high variance pays a lot of attention to training data (**overfitting**) but does not generalize well on new data.\n",
    "\n",
    "Such models perform very well on training data but have high errors on test data.\n",
    "\n",
    "To get a better model, we will reduce the degree of the polynomial. From observing the above scatterplot, we can assume a quadratic relation (2nd degree polynomial) between the size and the weight..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on TEST data:\n",
      "Mean squared error: 3.71\n",
      "Variance score: 0.82\n",
      "\n",
      "Model performance on TRAIN data:\n",
      "Mean squared error: 3.58\n",
      "Variance score: 0.87\n"
     ]
    }
   ],
   "source": [
    "poly_model = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
    "                  ('linear', LinearRegression())])\n",
    "poly_model = poly_model.fit(X_train, y_train)\n",
    "#print(poly_model.named_steps['linear'].coef_)\n",
    "\n",
    "Y_pred_poly = poly_model.predict(X_test)\n",
    "plt.plot(X_train, y_train, \"b.\", label=\"train data\")\n",
    "\n",
    "X_test2 = np.linspace(1, 7, 100)\n",
    "X_test2 = X_test2.reshape(-1, 1)\n",
    "Y_test2 = poly_model.predict(X_test2)\n",
    "plt.plot(X_test2, Y_test2, \"k-\", label=\"2nd deg poly\")\n",
    "plt.plot(X_test, y_test, \"ro\", label=\"test data\")\n",
    "\n",
    "y_true = - ((X_test2-5)**2) +25\n",
    "plt.plot(X_test2, y_true, \"g-\", label=\"true y\")\n",
    "\n",
    "plt.xlabel(\"weight [dag]\")\n",
    "plt.ylabel(\"size [cm]\")\n",
    "plt.title(\"2nd degree polynomial regression: Low bias, low variance\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "print(\"Model performance on TEST data:\")\n",
    "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(y_test, Y_pred_poly) ))\n",
    "print('Variance score: {:.2f}\\n'.format(r2_score(y_test, Y_pred_poly) ))\n",
    "\n",
    "y_pred_train2 = poly_model.predict(X_train)\n",
    "print(\"Model performance on TRAIN data:\")\n",
    "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(y_train, y_pred_train2) ))\n",
    "print('Variance score: {:.2f}'.format(r2_score(y_train, y_pred_train2) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and get just the right fit :)\n",
    "\n",
    "However, fitting the model to test data as we did above is not allowed! We did so to illustrate the bias-variance trade-off.\n",
    "\n",
    "Bellow, we show how to find the best model the right way. To do so, we only use training data and perform so called internal cross-validation. At the end, we test the final model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning the models with internal cross-validation\n"
     ]
    }
   ],
   "source": [
    "print(\"Fine tuning the models with internal cross-validation\")\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11    22.810423\n",
       "47    21.267879\n",
       "85    24.937030\n",
       "28    13.858575\n",
       "93    26.352526\n",
       "        ...    \n",
       "60    11.974135\n",
       "71    17.850108\n",
       "14     7.683603\n",
       "92    24.719025\n",
       "51    27.798327\n",
       "Name: size, Length: 70, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linear__fit_intercept': True, 'poly__degree': 2}\n",
      "Final testing...\n"
     ]
    }
   ],
   "source": [
    "pf = PolynomialFeatures()\n",
    "lr = LinearRegression()\n",
    "\n",
    "poly_model = Pipeline([('poly', pf), ('linear', lr)])\n",
    "parameters = {'poly__degree': range(2,20),\n",
    "              'linear__fit_intercept': [True, False]\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(poly_model, parameters, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    #print(np.sqrt(-mean_score), params)\n",
    "    pass\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"Final testing...\")\n",
    "final_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         10.6085037  -1.08420145]\n"
     ]
    }
   ],
   "source": [
    "lr_final = final_model['linear']\n",
    "cfs = lr_final.coef_\n",
    "print(cfs)\n",
    "\n",
    "x1 = np.linspace(1,6.9,100)\n",
    "y1 = - ((x1-5)**2) +25\n",
    "y2 = cfs[0] + cfs[1]*x1 + cfs[2]* x1**2\n",
    "plt.plot(x1, y1, \"g-\", label=\"true y\");\n",
    "plt.plot(x1, y2, \"b-\", label=\"model\");\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on TEST data:\n",
      "Mean squared error: 3.71\n",
      "Variance score: 0.82\n",
      "\n",
      "Model performance on TRAIN data:\n",
      "Mean squared error: 3.58\n",
      "Variance score: 0.87\n"
     ]
    }
   ],
   "source": [
    "y_final = final_model.predict(X_test)\n",
    "plt.plot(X_train, y_train, \"b.\", label=\"train data\")\n",
    "plt.plot(X_test, y_test, \"ro\", label=\"test data\")\n",
    "\n",
    "y_true = - ((X_test2-5)**2) +25\n",
    "plt.plot(X_test2, y_true, \"g-\", label=\"true y\")\n",
    "\n",
    "plt.xlabel(\"weight [dag]\")\n",
    "plt.ylabel(\"size [cm]\")\n",
    "plt.title(\"2nd degree polynomial regression: Low bias, low variance\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "print(\"Model performance on TEST data:\")\n",
    "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(y_test, y_final) ))\n",
    "print('Variance score: {:.2f}\\n'.format(r2_score(y_test, y_final) ))\n",
    "\n",
    "y_final_train = final_model.predict(X_train)\n",
    "print(\"Model performance on TRAIN data:\")\n",
    "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(y_train, y_final_train) ))\n",
    "print('Variance score: {:.2f}'.format(r2_score(y_train, y_final_train) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
